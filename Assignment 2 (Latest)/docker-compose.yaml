# ============================================================
# DOCKER COMPOSE ‚Äî Airflow + JupyterLab (Unified Data Paths)
# ============================================================

version: "3.9"

services:
  # ----------------------------------------------------------
  # üß† JUPYTERLAB (for exploration, model training, testing)
  # ----------------------------------------------------------
  jupyter:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: jupyter_lab
    ports:
      - "8888:8888"
    volumes:
      # Mount project root
      - .:/app
      # Share unified data paths with Airflow
      - ./datamart:/opt/airflow/datamart
      - ./model_bank:/opt/airflow/model_bank
      - ./utils:/opt/airflow/utils
      - ./raw_data:/opt/airflow/raw_data
      - ./data:/opt/airflow/data
    environment:
      - JUPYTER_ENABLE_LAB=yes
    command:
      [
        "jupyter", "lab",
        "--ip=0.0.0.0",
        "--port=8888",
        "--no-browser",
        "--allow-root",
        "--notebook-dir=/app",
        "--ServerApp.token=''",
        "--ServerApp.disable_check_xsrf=True"
      ]

  # ----------------------------------------------------------
  # ‚öôÔ∏è AIRFLOW INIT (DB + Admin user creation)
  # ----------------------------------------------------------
  airflow-init:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./model_bank:/opt/airflow/model_bank
      - ./datamart:/opt/airflow/datamart
      - ./utils:/opt/airflow/utils
      - ./raw_data:/opt/airflow/raw_data
      - ./data:/opt/airflow/data
      - ./model_inference:/opt/airflow/model_inference
    entrypoint: >
      /bin/bash -c "airflow db init &&
      airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com || true"

  # ----------------------------------------------------------
  # üåê AIRFLOW WEBSERVER (UI on localhost:8080)
  # ----------------------------------------------------------
  airflow-webserver:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      - airflow-init
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./model_bank:/opt/airflow/model_bank
      - ./datamart:/opt/airflow/datamart
      - ./utils:/opt/airflow/utils
      - ./raw_data:/opt/airflow/raw_data
      - ./data:/opt/airflow/data
      - ./model_inference:/opt/airflow/model_inference
    ports:
      - "8080:8080"
    command: webserver

  # ----------------------------------------------------------
  # ‚è±Ô∏è AIRFLOW SCHEDULER (DAG Orchestrator)
  # ----------------------------------------------------------
  airflow-scheduler:
    build:
      context: .
      dockerfile: Dockerfile.airflow
    depends_on:
      - airflow-init
    env_file:
      - .env
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=PAqBeGJLJTYFzVkOGHWIYXdLO7XdXz5yTdxAGJe9ezM=
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - PYTHONPATH=/opt/airflow
    volumes:
      - airflow_data:/opt/airflow
      - ./dags:/opt/airflow/dags
      - ./scripts:/opt/airflow/scripts
      - ./model_bank:/opt/airflow/model_bank
      - ./datamart:/opt/airflow/datamart
      - ./utils:/opt/airflow/utils
      - ./raw_data:/opt/airflow/raw_data
      - ./data:/opt/airflow/data
      - ./model_inference:/opt/airflow/model_inference
    command: scheduler

# --------------------------------------------------------------
# üóÇÔ∏è Named volume for Airflow metadata DB
# --------------------------------------------------------------
volumes:
  airflow_data:
