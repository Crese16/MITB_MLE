{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72483017-368c-416f-abed-a1efa50cd39f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/09 08:46:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting Gold Model Training with Proper OOT Split\n",
      "\n",
      "‚úÖ Spark Session Created: local[4]\n",
      "   - Version: 3.5.5\n",
      "   - Available Cores: 50\n",
      "\n",
      "üìÇ Found 25 monthly feature files.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Optimizing data partitioning...\n",
      "üìä Attempting to count rows...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Combined dataset: 104814 rows across 25 months.\n",
      "\n",
      "üßÆ Using 23 numeric features.\n",
      "\n",
      "üìÖ Available snapshots: ['2023-01-01', '2023-02-01', '2023-03-01', '2023-04-01', '2023-05-01', '2023-06-01', '2023-07-01', '2023-08-01', '2023-09-01', '2023-10-01', '2023-11-01', '2023-12-01', '2024-01-01', '2024-02-01', '2024-03-01', '2024-04-01', '2024-05-01', '2024-06-01', '2024-07-01', '2024-08-01', '2024-09-01', '2024-10-01', '2024-11-01', '2024-12-01', '2025-11-01']\n",
      "\n",
      "üéØ OOT Period: 2024-11-01 onwards (latest 3 months)\n",
      "üìö Training Period: Before 2024-11-01\n",
      "\n",
      "üìä Data Split:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Historical (for train/val/test): 93256 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  OOT (latest 3 months):           11558 rows\n",
      "\n",
      "üîÄ Splitting historical data into 70% train / 15% val / 15% test...\n",
      "\n",
      "üìä Historical Split Sizes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Train (70%):    65562 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Val (15%):      13909 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Test (15%):     13785 rows\n",
      "\n",
      "üîç Hyperparameter tuning on validation set...\n",
      "\n",
      "  Training LR_reg0.01... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AUC=0.7239 | PR=0.3514\n",
      "  Training LR_reg0.05... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AUC=0.7238 | PR=0.3511\n",
      "‚úÖ AUC=0.7237 | PR=0.3508\n",
      "  Training RF_depth6... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AUC=0.7261 | PR=0.3559\n",
      "  Training RF_depth8... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AUC=0.7373 | PR=0.3682\n",
      "\n",
      "üèÜ Best model (from validation): RandomForest (param=8, ValAUC=0.7373)\n",
      "\n",
      "üîÑ Training final model on full training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Evaluating on historical test set (15%)...\n",
      "   Historical Test ‚Äî AUC=0.7269, PR=0.3655\n",
      "\n",
      "üìä Evaluating on OOT set (latest 3 months)...\n",
      "   OOT Test ‚Äî AUC=0.6837, PR=0.3661\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved model ‚Üí /opt/airflow/utils/model_bank/RandomForest_OOT_model_20251109_085136\n",
      "üìä Saved metrics ‚Üí /opt/airflow/utils/model_bank/oot_model_metrics_20251109_085136.csv\n",
      "\n",
      "============================================================\n",
      "üìà FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Best Model: RandomForest (param=8)\n",
      "\n",
      "Validation AUC:        0.7373\n",
      "Historical Test AUC:   0.7269 | PR=0.3655\n",
      "OOT Test AUC:          0.6837 | PR=0.3661\n",
      "============================================================\n",
      "\n",
      "‚úÖ OOT Model Training Completed Successfully.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# =============================================================\n",
    "# GOLD MODEL TRAINING ‚Äî Proper OOT Split (Latest 3 Months)\n",
    "# =============================================================\n",
    "\n",
    "import os, re, glob\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1Ô∏è‚É£ Spark Init (Memory Safe + Local Mode Fix)\n",
    "# -------------------------------------------------------------\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"Gold_Model_Training_OOT\")\n",
    "    .master(\"local[4]\")\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"2000\")\n",
    "    .config(\"spark.driver.memory\", \"8g\")\n",
    "    .config(\"spark.executor.memory\", \"8g\")\n",
    "    .config(\"spark.memory.fraction\", \"0.8\")\n",
    "    .config(\"spark.memory.storageFraction\", \"0.3\")\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"50\")\n",
    "    .config(\"spark.default.parallelism\", \"50\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"3g\")\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"\\nüöÄ Starting Gold Model Training with Proper OOT Split\\n\")\n",
    "print(f\"‚úÖ Spark Session Created: {spark.sparkContext.master}\")\n",
    "print(f\"   - Version: {spark.version}\")\n",
    "print(f\"   - Available Cores: {spark.sparkContext.defaultParallelism}\\n\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 2Ô∏è‚É£ Paths\n",
    "# -------------------------------------------------------------\n",
    "BASE_DIR = \"/opt/airflow\" if os.path.exists(\"/opt/airflow\") else \".\"\n",
    "FEATURE_PATH = os.path.join(BASE_DIR, \"datamart/gold/feature_store\")\n",
    "LABEL_PATH = os.path.join(BASE_DIR, \"datamart/gold/label_store\")\n",
    "MODEL_BANK = os.path.join(BASE_DIR, \"utils/model_bank\")\n",
    "os.makedirs(MODEL_BANK, exist_ok=True)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 3Ô∏è‚É£ Gather All Monthly Parquets\n",
    "# -------------------------------------------------------------\n",
    "feature_files = sorted(glob.glob(os.path.join(FEATURE_PATH, \"gold_feature_store_*.parquet\")))\n",
    "label_files = sorted(glob.glob(os.path.join(LABEL_PATH, \"gold_label_store_*.parquet\")))\n",
    "\n",
    "if not feature_files or not label_files:\n",
    "    raise FileNotFoundError(\"‚ùå No Gold feature or label store found. Run main.py first.\")\n",
    "\n",
    "print(f\"üìÇ Found {len(feature_files)} monthly feature files.\\n\")\n",
    "\n",
    "all_df = None\n",
    "for fpath in feature_files:\n",
    "    tag_match = re.search(r\"(\\d{4}_\\d{2}_\\d{2})\", fpath)\n",
    "    if not tag_match:\n",
    "        continue\n",
    "    tag = tag_match.group(1)\n",
    "    lpath = os.path.join(LABEL_PATH, f\"gold_label_store_{tag}.parquet\")\n",
    "    if not os.path.exists(lpath):\n",
    "        continue\n",
    "\n",
    "    f_df = spark.read.parquet(fpath)\n",
    "    l_df = spark.read.parquet(lpath)\n",
    "\n",
    "    # Avoid ambiguous label join\n",
    "    g_df = (\n",
    "        f_df.alias(\"f\")\n",
    "        .join(l_df.select(\"Customer_ID\", F.col(\"label\").alias(\"label_y\")), \"Customer_ID\", \"inner\")\n",
    "        .drop(\"label\")\n",
    "        .withColumnRenamed(\"label_y\", \"label\")\n",
    "        .withColumn(\"snapshot_tag\", F.lit(tag))\n",
    "    )\n",
    "\n",
    "    all_df = g_df if all_df is None else all_df.unionByName(g_df, allowMissingColumns=True)\n",
    "\n",
    "if all_df is None:\n",
    "    raise ValueError(\"‚ùå No valid feature-label pairs found.\")\n",
    "\n",
    "# Optimize data partitioning\n",
    "print(\"üîÑ Optimizing data partitioning...\")\n",
    "all_df = all_df.repartition(20)\n",
    "all_df = all_df.cache()\n",
    "\n",
    "print(\"üìä Attempting to count rows...\")\n",
    "try:\n",
    "    total_rows = all_df.count()\n",
    "    print(f\"‚úÖ Combined dataset: {total_rows} rows across {len(feature_files)} months.\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Count failed: {str(e)[:100]}\")\n",
    "    print(\"   Continuing without count...\\n\")\n",
    "    all_df.unpersist()\n",
    "    all_df = all_df.cache()\n",
    "    total_rows = \"Unknown\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 4Ô∏è‚É£ Feature Cleaning\n",
    "# -------------------------------------------------------------\n",
    "exclude_cols = {\"Customer_ID\", \"snapshot_date\", \"gold_processing_date\", \"label\", \"snapshot_tag\"}\n",
    "feature_cols = [\n",
    "    c for (c, t) in all_df.dtypes\n",
    "    if (t in [\"int\", \"double\", \"float\", \"bigint\"]) and (c not in exclude_cols)\n",
    "]\n",
    "\n",
    "for c in feature_cols:\n",
    "    all_df = all_df.withColumn(\n",
    "        c, F.when(F.col(c).isin(float(\"inf\"), float(\"-inf\")), None).otherwise(F.col(c))\n",
    "    )\n",
    "all_df = all_df.fillna(0, subset=feature_cols)\n",
    "all_df = all_df.withColumn(\"label\", F.col(\"label\").cast(DoubleType()))\n",
    "\n",
    "print(f\"üßÆ Using {len(feature_cols)} numeric features.\\n\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 5Ô∏è‚É£ Parse snapshot_tag ‚Üí snapshot_date & Identify OOT Period\n",
    "# -------------------------------------------------------------\n",
    "def parse_tag_from_path(path):\n",
    "    match = re.search(r\"(\\d{4}_\\d{2}_\\d{2})\", os.path.basename(path))\n",
    "    if match:\n",
    "        try:\n",
    "            return datetime.strptime(match.group(1), \"%Y_%m_%d\")\n",
    "        except:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "tags = sorted([parse_tag_from_path(f) for f in feature_files if parse_tag_from_path(f) is not None])\n",
    "if not tags:\n",
    "    raise ValueError(\"‚ùå No valid snapshot tags found in feature_store filenames.\")\n",
    "\n",
    "all_df = all_df.withColumn(\n",
    "    \"snapshot_date\",\n",
    "    F.to_date(F.regexp_extract(F.col(\"snapshot_tag\"), r\"(\\d{4}_\\d{2}_\\d{2})\", 1), \"yyyy_MM_dd\")\n",
    ")\n",
    "\n",
    "print(f\"üìÖ Available snapshots: {[t.strftime('%Y-%m-%d') for t in tags]}\")\n",
    "\n",
    "# Get latest 3 months for OOT\n",
    "if len(tags) < 4:\n",
    "    raise ValueError(f\"‚ùå Need at least 4 months of data. Found only {len(tags)} months.\")\n",
    "\n",
    "oot_cutoff = tags[-3]  # Start of latest 3 months\n",
    "print(f\"\\nüéØ OOT Period: {oot_cutoff.strftime('%Y-%m-%d')} onwards (latest 3 months)\")\n",
    "print(f\"üìö Training Period: Before {oot_cutoff.strftime('%Y-%m-%d')}\\n\")\n",
    "\n",
    "# Split: Historical data vs OOT\n",
    "historical_df = all_df.filter(F.col(\"snapshot_date\") < F.lit(oot_cutoff)).cache()\n",
    "oot_df = all_df.filter(F.col(\"snapshot_date\") >= F.lit(oot_cutoff)).cache()\n",
    "\n",
    "# Optimize partitions\n",
    "historical_df = historical_df.repartition(15)\n",
    "oot_df = oot_df.repartition(5)\n",
    "\n",
    "print(\"üìä Data Split:\")\n",
    "try:\n",
    "    hist_count = historical_df.count()\n",
    "    print(f\"  Historical (for train/val/test): {hist_count} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  Historical: [count failed - {str(e)[:50]}]\")\n",
    "    hist_count = \"N/A\"\n",
    "\n",
    "try:\n",
    "    oot_count = oot_df.count()\n",
    "    print(f\"  OOT (latest 3 months):           {oot_count} rows\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"  OOT: [count failed - {str(e)[:50]}]\\n\")\n",
    "    oot_count = \"N/A\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 6Ô∏è‚É£ Split Historical Data: 70/15/15 (Train/Val/Test)\n",
    "# -------------------------------------------------------------\n",
    "print(\"üîÄ Splitting historical data into 70% train / 15% val / 15% test...\")\n",
    "\n",
    "# Add random column for splitting\n",
    "historical_df = historical_df.withColumn(\"rand\", F.rand(seed=42))\n",
    "\n",
    "train_df = historical_df.filter(F.col(\"rand\") < 0.70).drop(\"rand\").cache()\n",
    "val_df = historical_df.filter((F.col(\"rand\") >= 0.70) & (F.col(\"rand\") < 0.85)).drop(\"rand\").cache()\n",
    "test_df = historical_df.filter(F.col(\"rand\") >= 0.85).drop(\"rand\").cache()\n",
    "\n",
    "# Optimize partitions\n",
    "train_df = train_df.repartition(10)\n",
    "val_df = val_df.repartition(3)\n",
    "test_df = test_df.repartition(3)\n",
    "\n",
    "print(\"\\nüìä Historical Split Sizes:\")\n",
    "try:\n",
    "    train_count = train_df.count()\n",
    "    print(f\"  Train (70%):    {train_count} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  Train (70%):    [count failed - {str(e)[:50]}]\")\n",
    "    train_count = \"N/A\"\n",
    "\n",
    "try:\n",
    "    val_count = val_df.count()\n",
    "    print(f\"  Val (15%):      {val_count} rows\")\n",
    "except Exception as e:\n",
    "    print(f\"  Val (15%):      [count failed - {str(e)[:50]}]\")\n",
    "    val_count = \"N/A\"\n",
    "\n",
    "try:\n",
    "    test_count = test_df.count()\n",
    "    print(f\"  Test (15%):     {test_count} rows\\n\")\n",
    "except Exception as e:\n",
    "    print(f\"  Test (15%):     [count failed - {str(e)[:50]}]\\n\")\n",
    "    test_count = \"N/A\"\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 7Ô∏è‚É£ Assemble + Scale\n",
    "# -------------------------------------------------------------\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features_raw\", handleInvalid=\"skip\")\n",
    "scaler = StandardScaler(inputCol=\"features_raw\", outputCol=\"features\", withStd=True, withMean=False)\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 8Ô∏è‚É£ Hyperparameter Tuning on Validation Set\n",
    "# -------------------------------------------------------------\n",
    "def train_and_eval(model_name, model_obj, train_df, val_df):\n",
    "    try:\n",
    "        pipeline = Pipeline(stages=[assembler, scaler, model_obj])\n",
    "        print(f\"  Training {model_name}...\", end=\" \", flush=True)\n",
    "        model = pipeline.fit(train_df)\n",
    "        preds = model.transform(val_df)\n",
    "        \n",
    "        eval_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "        eval_pr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n",
    "        auc = eval_auc.evaluate(preds)\n",
    "        pr = eval_pr.evaluate(preds)\n",
    "        \n",
    "        print(f\"‚úÖ AUC={auc:.4f} | PR={pr:.4f}\")\n",
    "        return model, auc, pr\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå FAILED: {str(e)[:100]}\")\n",
    "        return None, 0.0, 0.0\n",
    "\n",
    "print(\"üîç Hyperparameter tuning on validation set...\\n\")\n",
    "results = []\n",
    "\n",
    "# Logistic Regression grid\n",
    "for reg in [0.01, 0.05, 0.1]:\n",
    "    lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=50, regParam=reg)\n",
    "    _, auc, pr = train_and_eval(f\"LR_reg{reg}\", lr, train_df, val_df)\n",
    "    if auc > 0:\n",
    "        results.append((\"LogisticRegression\", reg, auc, pr))\n",
    "\n",
    "# Random Forest grid\n",
    "for depth in [6, 8]:\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        numTrees=50,\n",
    "        maxDepth=depth,\n",
    "        maxBins=32\n",
    "    )\n",
    "    _, auc, pr = train_and_eval(f\"RF_depth{depth}\", rf, train_df, val_df)\n",
    "    if auc > 0:\n",
    "        results.append((\"RandomForest\", depth, auc, pr))\n",
    "\n",
    "if not results:\n",
    "    raise ValueError(\"‚ùå All models failed to train. Check memory and data quality.\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 9Ô∏è‚É£ Select Best Model & Evaluate on Historical Test Set\n",
    "# -------------------------------------------------------------\n",
    "best_model_name, best_param, best_auc, _ = max(results, key=lambda x: x[2])\n",
    "print(f\"\\nüèÜ Best model (from validation): {best_model_name} (param={best_param}, ValAUC={best_auc:.4f})\")\n",
    "\n",
    "if best_model_name == \"LogisticRegression\":\n",
    "    final_model = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", regParam=best_param, maxIter=50)\n",
    "else:\n",
    "    final_model = RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"label\", \n",
    "        numTrees=50, \n",
    "        maxDepth=best_param,\n",
    "        maxBins=32\n",
    "    )\n",
    "\n",
    "print(\"\\nüîÑ Training final model on full training set...\")\n",
    "pipeline = Pipeline(stages=[assembler, scaler, final_model])\n",
    "final_fit = pipeline.fit(train_df)\n",
    "\n",
    "print(\"üìä Evaluating on historical test set (15%)...\")\n",
    "pred_test = final_fit.transform(test_df)\n",
    "\n",
    "eval_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "eval_pr = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderPR\")\n",
    "auc_test = eval_auc.evaluate(pred_test)\n",
    "pr_test = eval_pr.evaluate(pred_test)\n",
    "\n",
    "print(f\"   Historical Test ‚Äî AUC={auc_test:.4f}, PR={pr_test:.4f}\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# üîü Final Evaluation on OOT (Latest 3 Months)\n",
    "# -------------------------------------------------------------\n",
    "print(\"\\nüìä Evaluating on OOT set (latest 3 months)...\")\n",
    "pred_oot = final_fit.transform(oot_df)\n",
    "\n",
    "auc_oot = eval_auc.evaluate(pred_oot)\n",
    "pr_oot = eval_pr.evaluate(pred_oot)\n",
    "\n",
    "print(f\"   OOT Test ‚Äî AUC={auc_oot:.4f}, PR={pr_oot:.4f}\\n\")\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Save Model + Metrics\n",
    "# -------------------------------------------------------------\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "model_path = os.path.join(MODEL_BANK, f\"{best_model_name}_OOT_model_{timestamp}\")\n",
    "final_fit.write().overwrite().save(model_path)\n",
    "\n",
    "metrics_path = os.path.join(MODEL_BANK, f\"oot_model_metrics_{timestamp}.csv\")\n",
    "pd.DataFrame(\n",
    "    [[best_model_name, best_param, best_auc, auc_test, pr_test, auc_oot, pr_oot, \n",
    "      str(train_count), str(val_count), str(test_count), str(oot_count)]],\n",
    "    columns=[\"Model\", \"BestParam\", \"ValAUC\", \"HistTestAUC\", \"HistTestPR\", \"OOT_AUC\", \"OOT_PR\",\n",
    "             \"TrainRows\", \"ValRows\", \"HistTestRows\", \"OOT_Rows\"]\n",
    ").to_csv(metrics_path, index=False)\n",
    "\n",
    "print(f\"üíæ Saved model ‚Üí {model_path}\")\n",
    "print(f\"üìä Saved metrics ‚Üí {metrics_path}\")\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìà FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Best Model: {best_model_name} (param={best_param})\")\n",
    "print(f\"\\nValidation AUC:        {best_auc:.4f}\")\n",
    "print(f\"Historical Test AUC:   {auc_test:.4f} | PR={pr_test:.4f}\")\n",
    "print(f\"OOT Test AUC:          {auc_oot:.4f} | PR={pr_oot:.4f}\")\n",
    "print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "# Cleanup\n",
    "all_df.unpersist()\n",
    "historical_df.unpersist()\n",
    "train_df.unpersist()\n",
    "val_df.unpersist()\n",
    "test_df.unpersist()\n",
    "oot_df.unpersist()\n",
    "\n",
    "spark.stop()\n",
    "print(\"‚úÖ OOT Model Training Completed Successfully.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e516d-ba4e-4a90-9cc0-e4e32f323b45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28478358-87b9-445c-92d9-16a6a132aba8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29243b30-5ab2-493f-87c8-ca7423ead06c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7000d3b1-e28b-4f76-a843-a7cbd7c6782a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
